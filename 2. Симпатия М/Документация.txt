Данные нейросети были одними из первых, которые я написал. Они определяют, сформируется ли симпатия к человеку по шести признакам:
man - мужчина        Самое важное
educ - образование   Важное
money - богатство    Сильно отрицательное
tall - высокий       Отрицательное
calm - спокойный     Желательно
glasses - очки       Желательно

Эти шесть признаков подаются последовательно через пробел и принимают значение 0, если этого нет, и 1, если это есть.

Я вручную задавал условия, при которых происходит активация нейронов скрытого слоя. На входном слое - 6 нейронов, по 1 для каждого признака (нейрон имеет значение 1, если "это" есть, и значение 0, если "этого" нет). В скрытом слое содержится 3 нейрона. Каждый из них имеет следующие условия активации: 

нейрон 1: если спокойный + в очках = 1, в противном случае 0
нейрон 2: не богатый + не высокий = 0, в противном случае 1
нейрон 3: образованный + мужчина = 1, в противном случае 0

Исходя из этих условий, я вручную подбирал и расставлял веса для связей нейронов входного и скрытого слоя. 
На выходном слое содержится один нейрон. Он выводит итоговый ответ модели: "Нравится", если функция активации выдала 1, и "Не нравится", если функция активации выдала 0. 
Функция активации одинакова для скрытого и выходного слоя и представляет собой обычную пороговую функция (если итоговая сумма на нейроне >= 0.5, то нейрон активируется, в противном случае - нет).

Исходя из приведенных выше данных, симпатия сформируется при следующих условиях:
1. Обязательно - мужчина + образованный (если нейрон 3 не загорится, симпатия ни при каких условиях не будет сформирована)
2. Обязательно - НЕ богатый + НЕ высокий (в противном случае нейрон 2 активируется, что сразу же гарантирует то, что симпатия НЕ сформируется)

Нейрон 1 не является обязательным для формирования симпатии. Параметры "очки" и "спокойствие" не являются решающими, однако их наличие будет свидетельствовать о более уверенном формировании симпатии.


В версии 2 я отошел от ручного распределения весов для путей между нейронами. Для их автоматической корректировки был применен алгоритм обратного распространения ошибки (backpropagation). По условиям, составленным мною ранее, я составил список примеров для обучения нейросети. Все эти примеры я занес в файл и использовал для обучения. 

Алгоритм backpropagation позволил расставить веса случайным образом, ведь далее, в ходе обучения, нейросеть сама подстраивается под примеры и корректирует веса. 

(Стоит также отметить, что в моделях версии 2 я расставлял одинаковые изначальные веса для всех связей. Это было возможно, поскольку нейросеть имела четкие параметры на вход, которые не менялись. Это позволяло программе находить закономерности даже в условиях изначально равных весов)

Поскольку для нахождения градиента для алгоритма backpropagation необходимо взять производную от функции активации, я был вынужден изменить ее на гиперболический тангенс. Таким образом, теперь параметры на вход состояли из -1 и 1 (можно было бы взять логическую функцию, оставив 0 и 1, но я захотел взять тангенс). 

Также мной была добавлена функция вычисления ошибки.  Данная функция позволяет в режиме реального времени наблюдать за тем, как происходит процесс обучения и какую ошибку совершает нейросеть. Данная опция позволила более точно подогнать другие параметры, такие как количество эпох (N) и коэфициент обучения (lmd).