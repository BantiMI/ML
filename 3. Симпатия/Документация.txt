На данной версии нейросети я впервые применил алгоритм backpropagation. 

На вход нейросеть принимает 3 параметра: красоту, наличие дома, любовь к музыке (-1, если нет и 1, если есть). 

В качестве функции активации для скрытого и выходного слоя используется функция гиперболического тангенса. Гиперболический тангенс наиболее чувствителен к границам то есть к -1 и 1. Именно поэтому входные данные передаются на входной слой в значении -1, если нет, и 1, если есть.

Алгоритм Backpropagation позволил расставить изначальные веса рандомно. Конкретно в данной модели я выставил одинаковые изначальные веса всех связей. Это было возможным, поскольку данные подавались упорядоченно (один и тот же параметр на один и тот же нейрон). Благодаря этому нейросеть могла находить закономерности даже при условии одинаковых изначальных весов. 

На этой нейросети я тестировал алгоритм обратного распространения ошибки, перед его внедрением в более масштабные нейронные сети. 

Поскольку для активации нейронов и скрытого, и выходного слоя применялась одна и т же функция активации - гиперболический тангенс, то на выход нейросеть выдает не -1 или 1, а приближенные к этим числам значения (требуется округление). 

Вместо гиперболического тангенса можно было бы взять логическую функцию. В таком случае сеть была бы чувствительна к 0 и 1. 